{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Sep 25 22:29:37 2018\n",
    "\n",
    "@author: jack.lingheng.meng\n",
    "\"\"\"\n",
    "import os\n",
    "#os.system('module load nixpkgs/16.09  gcc/5.4.0  cuda/8.0.44  cudnn/7.0 opencv/3.3.0  boost/1.65.1 openblas/0.2.20 hdf5/1.8.18 leveldb/1.18 mkl-dnn/0.14 python/3.5.2')\n",
    "os.system('module load nixpkgs/16.09  gcc/5.4.0  cuda/9.0.176  cudnn/7.0 opencv/3.3.0  boost/1.65.1 openblas/0.2.20 hdf5/1.8.18 leveldb/1.18 mkl-dnn/0.14 python/3.5.2')\n",
    "\n",
    "os.system('cd ~')\n",
    "os.system('source openposeEnv_Python3/bin/activate')\n",
    "os.system('export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/openpose_python_lib/lib:$HOME/openpose_python_lib/python/openpose:$HOME/caffe/build/lib:/cvmfs/soft.computecanada.ca/easybuild/software/2017/avx2/Compiler/gcc5.4/boost/1.65.1/lib')\n",
    "import sys\n",
    "sys.path.insert(0,r'/home/lingheng/openpose_python_lib/python/openpose') \n",
    "try:\n",
    "    from openpose import *\n",
    "except:\n",
    "    raise Exception('Error: OpenPose library could not be found. Did you enable `BUILD_PYTHON` in CMake and have this Python script in the right folder?')\n",
    "params = dict()\n",
    "params[\"logging_level\"] = 3\n",
    "params[\"output_resolution\"] = \"-1x-1\"\n",
    "params[\"net_resolution\"] = \"-1x736\" # if crop video, this should be changged and must be mutplies of 16.\n",
    "params[\"model_pose\"] = \"COCO\"\n",
    "params[\"alpha_pose\"] = 0.6\n",
    "params[\"scale_gap\"] = 0.25\n",
    "params[\"scale_number\"] = 4\n",
    "params[\"render_threshold\"] = 0.05\n",
    "# If GPU version is built, and multiple GPUs are available, set the ID here\n",
    "params[\"num_gpu_start\"] = 1\n",
    "params[\"disable_blending\"] = False\n",
    "# Ensure you point to the correct path where models are located\n",
    "params[\"default_model_folder\"] = \"/home/lingheng/openpose/models/\"\n",
    "# Construct OpenPose object allocates GPU memory\n",
    "openpose = OpenPose(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Python\n",
    "# It requires OpenCV installed for Python\n",
    "import sys\n",
    "import csv\n",
    "import cv2\n",
    "import os\n",
    "from sys import platform\n",
    "import argparse\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # Must be before importing matplotlib.pyplot or pylab!\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import mode\n",
    "\n",
    "import pdb\n",
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "# Remember to add your installation path here\n",
    "# Option b\n",
    "# If you run `make install` (default path is `/usr/local/python` for Ubuntu), you can also access the OpenPose/python module from there. This will install OpenPose and the python library at your desired installation path. Ensure that this is in your python path in order to use it.\n",
    "sys.path.insert(0,r'/home/lingheng/openpose_python_lib/python/openpose') \n",
    "\n",
    "# Parameters for OpenPose. Take a look at C++ OpenPose example for meaning of components. Ensure all below are filled\n",
    "try:\n",
    "    from openpose import *\n",
    "except:\n",
    "    raise Exception('Error: OpenPose library could not be found. Did you enable `BUILD_PYTHON` in CMake and have this Python script in the right folder?')\n",
    "params = dict()\n",
    "params[\"logging_level\"] = 3\n",
    "params[\"output_resolution\"] = \"-1x-1\"\n",
    "params[\"net_resolution\"] = \"-1x736\" # if crop video, this should be changged and must be mutplies of 16.\n",
    "params[\"model_pose\"] = \"MPI_4_layers\"#\"COCO\" #\"MPI\" #\"BODY_25\" #\"MPI_4_layers\"\n",
    "params[\"alpha_pose\"] = 0.6\n",
    "params[\"scale_gap\"] = 0.25\n",
    "params[\"scale_number\"] = 4\n",
    "params[\"render_threshold\"] = 0.05\n",
    "# If GPU version is built, and multiple GPUs are available, set the ID here\n",
    "params[\"num_gpu_start\"] = 1\n",
    "params[\"disable_blending\"] = False\n",
    "# Ensure you point to the correct path where models are located\n",
    "params[\"default_model_folder\"] = \"/home/lingheng/openpose/models/\"\n",
    "# Construct OpenPose object allocates GPU memory\n",
    "openpose = OpenPose(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw frames per second: 23.056189901338755\n",
      "Frame width:1152, Frame height:972.\n",
      "Total frame number: 41445.0\n",
      "Processing frame: 0\n",
      "Processing frame: 200\n",
      "Processing frame: 400\n",
      "Processing frame: 600\n",
      "Processing frame: 800\n",
      "Processing frame: 1000\n",
      "Processing frame: 1200\n",
      "Processing frame: 1400\n",
      "Processing frame: 1600\n",
      "Processing frame: 1800\n",
      "Processing frame: 2000\n",
      "Processing frame: 2200\n",
      "Processing frame: 2400\n",
      "Processing frame: 2600\n",
      "Processing frame: 2800\n",
      "Processing frame: 3000\n",
      "Processing frame: 3200\n",
      "Processing frame: 3400\n",
      "Processing frame: 3600\n",
      "Processing frame: 3800\n",
      "Processing frame: 4000\n",
      "Processing frame: 4200\n",
      "Processing frame: 4400\n",
      "Processing frame: 4600\n",
      "Processing frame: 4800\n",
      "Processing frame: 5000\n",
      "Processing frame: 5200\n",
      "Processing frame: 5400\n",
      "Processing frame: 5600\n",
      "Processing frame: 5800\n",
      "Processing frame: 6000\n",
      "Processing frame: 6200\n",
      "Processing frame: 6400\n",
      "Processing frame: 6600\n",
      "Processing frame: 6800\n",
      "Processing frame: 7000\n",
      "Processing frame: 7200\n",
      "Processing frame: 7400\n",
      "Processing frame: 7600\n",
      "Processing frame: 7800\n",
      "Processing frame: 8000\n",
      "Processing frame: 8200\n",
      "Processing frame: 8400\n",
      "Processing frame: 8600\n",
      "Processing frame: 8800\n",
      "Processing frame: 9000\n",
      "Processing frame: 9200\n",
      "Processing frame: 9400\n",
      "Processing frame: 9600\n",
      "Processing frame: 9800\n",
      "Processing frame: 10000\n",
      "Processing frame: 10200\n",
      "Processing frame: 10400\n",
      "Processing frame: 10600\n",
      "Processing frame: 10800\n",
      "Processing frame: 11000\n",
      "Processing frame: 11200\n",
      "Processing frame: 11400\n",
      "Processing frame: 11600\n",
      "Processing frame: 11800\n",
      "Processing frame: 12000\n",
      "Processing frame: 12200\n",
      "Processing frame: 12400\n",
      "Processing frame: 12600\n",
      "Processing frame: 12800\n",
      "Processing frame: 13000\n",
      "Processing frame: 13200\n",
      "Processing frame: 13400\n",
      "Processing frame: 13600\n",
      "Processing frame: 13800\n",
      "Processing frame: 14000\n",
      "Processing frame: 14200\n",
      "Processing frame: 14400\n",
      "Processing frame: 14600\n",
      "Processing frame: 14800\n",
      "Processing frame: 15000\n",
      "Processing frame: 15200\n",
      "Processing frame: 15400\n",
      "Processing frame: 15600\n",
      "Processing frame: 15800\n",
      "Processing frame: 16000\n",
      "Processing frame: 16200\n",
      "Processing frame: 16400\n",
      "Processing frame: 16600\n",
      "Processing frame: 16800\n",
      "Processing frame: 17000\n",
      "Processing frame: 17200\n",
      "Processing frame: 17400\n",
      "Processing frame: 17600\n",
      "Processing frame: 17800\n",
      "Processing frame: 18000\n",
      "Processing frame: 18200\n",
      "Processing frame: 18400\n",
      "Processing frame: 18600\n",
      "Processing frame: 18800\n",
      "Processing frame: 19000\n",
      "Processing frame: 19200\n",
      "Processing frame: 19400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9c9da2f316d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# Output keypoints and the image with the human skeleton blended on it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m#    (num_people, 25_keypoints, x_y_confidence) = keypoints_whole_interest_area.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mkeypoints_whole_interest_area\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_image_whole_interest_area\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenpose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_cropped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# 2. Core Interest Area\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openpose_python_lib/python/openpose/openpose.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, display)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mdisplayImage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplayImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # construct the argument parser and parse the arguments\n",
    "#     ap = argparse.ArgumentParser()\n",
    "#     ap.add_argument(\"-v\", \"--video\", default='/home/lingheng/project/lingheng/ROM_raw_videos/Camera1_test.mp4', help=\"path to the video file\")\n",
    "#     ap.add_argument(\"-o\", \"--output_directory\", default='/home/lingheng/project/lingheng/ROM_processed_videos', help=\"directory to save processed video\")\n",
    "    \n",
    "#     args = vars(ap.parse_args())\n",
    "\n",
    "# construct the argument parser and parse the arguments\n",
    "args = dict()\n",
    "args['video']='/home/lingheng/project/lingheng/ROM_Video_Process/ROM_raw_videos/Sep_19/Camera1_Sep_19_1400_1430_Parameterized_Learning_Agent.mp4'\n",
    "args['output_directory']='/home/lingheng/project/lingheng/ROM_Video_Process/ROM_processed_and_tagged_videos/Sep_19'\n",
    "\n",
    "if args.get(\"video\", None) is None:\n",
    "    raise Error(\"No input video!!\")\n",
    "# otherwise, we are reading from a video file\n",
    "else:\n",
    "    camera = cv2.VideoCapture(args[\"video\"])\n",
    "########################################################################\n",
    "#                         Estimate Occupancy                           #\n",
    "########################################################################\n",
    "# frames per second (fps) in the raw video\n",
    "fps = camera.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = 1\n",
    "print(\"Raw frames per second: {0}\".format(fps))\n",
    "# prepare to save video\n",
    "(grabbed, frame) = camera.read()\n",
    "## downsample frame\n",
    "#downsample_rate = 0.5\n",
    "#frame = cv2.resize(frame,None,fx=downsample_rate, fy=downsample_rate, interpolation = cv2.INTER_LINEAR)\n",
    "# crop frame\n",
    "original_h, original_w, channels= frame.shape\n",
    "top_edge = int(original_h*(1/10))\n",
    "down_edge = int(original_h*1)\n",
    "left_edge = int(original_w*(1/5))\n",
    "right_edge = int(original_w*(4/5))\n",
    "frame_cropped = frame[top_edge:down_edge,left_edge:right_edge,:].copy() # must use copy(), otherwise slice only return address i.e. not hard copy\n",
    "\n",
    "cropped_h, cropped_w, channels = frame_cropped.shape\n",
    "fwidth = cropped_w \n",
    "fheight = cropped_h\n",
    "print(\"Frame width:{}, Frame height:{}.\".format(cropped_w , cropped_h))\n",
    "# Define the polygon of Core Interest Area\n",
    "point_1 = [int(0.17 * cropped_w), int(0.20 * cropped_h)]\n",
    "point_2 = [int(0.17 * cropped_w), int(0.62 * cropped_h)]\n",
    "point_3 = [int(0.44 * cropped_w), int(0.82 * cropped_h)]\n",
    "point_4 = [int(0.61 * cropped_w), int(0.72 * cropped_h)]\n",
    "point_5 = [int(0.61 * cropped_w), int(0.20 * cropped_h)]\n",
    "core_interest_area_polygon = np.array([point_1,point_2,point_3,point_4,point_5])\n",
    "\n",
    "# get output video file name\n",
    "file_path = args[\"video\"].split('/')\n",
    "file_name, _= file_path[-1].split('.')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "\n",
    "if not os.path.exists(args['output_directory']):\n",
    "    os.makedirs(args['output_directory'])\n",
    "output_video_filename = os.path.join(args['output_directory'],'{}_processed_{}.avi'.format(file_name,params[\"model_pose\"]))\n",
    "out_camera_frame_whole = cv2.VideoWriter(output_video_filename,fourcc, fps, (fwidth,fheight))\n",
    "\n",
    "# get output estimated occupancy file name\n",
    "out_occupancy_whole = os.path.join(args['output_directory'],'{}_processed_occupancy_whole_{}.csv'.format(file_name,params[\"model_pose\"]))\n",
    "out_occupancy_core = os.path.join(args['output_directory'],'{}_processed_occupancy_core_{}.csv'.format(file_name,params[\"model_pose\"]))\n",
    "out_occupancy_margin = os.path.join(args['output_directory'],'{}_processed_occupancy_margin_{}.csv'.format(file_name,params[\"model_pose\"]))\n",
    "with open(out_occupancy_whole, 'a') as csv_datafile:\n",
    "    fieldnames = ['Time', 'Occupancy']\n",
    "    writer = csv.DictWriter(csv_datafile, fieldnames = fieldnames)\n",
    "    writer.writeheader()\n",
    "with open(out_occupancy_core, 'a') as csv_datafile:\n",
    "    fieldnames = ['Time', 'Occupancy']\n",
    "    writer = csv.DictWriter(csv_datafile, fieldnames = fieldnames)\n",
    "    writer.writeheader()    \n",
    "with open(out_occupancy_margin, 'a') as csv_datafile:\n",
    "    fieldnames = ['Time', 'Occupancy']\n",
    "    writer = csv.DictWriter(csv_datafile, fieldnames = fieldnames)\n",
    "    writer.writeheader()      \n",
    "\n",
    "# loop over the frames of the video\n",
    "total_frame_number = camera.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "print('Total frame number: {}'.format(total_frame_number))\n",
    "CALCULATE_EVERY_X_FRAME = 10\n",
    "ignore_frame_count = CALCULATE_EVERY_X_FRAME\n",
    "for frame_count in range(int(total_frame_number)):\n",
    "    if frame_count % 200 == 0:\n",
    "        print('Processing frame: {}'.format(frame_count))\n",
    "    (grabbed, frame) = camera.read()\n",
    "    # TODO: it's not necessary to process every frame.\n",
    "    #       Observation is received in 10hz i.e. each observation takes 100millisecond.\n",
    "    #       Each frame take 33millisecond, so we could estimate occupancy every 3 frame.\n",
    "    if ignore_frame_count == CALCULATE_EVERY_X_FRAME:\n",
    "        ignore_frame_count = 1\n",
    "    else:\n",
    "        ignore_frame_count += 1\n",
    "        continue\n",
    "    if grabbed == True:\n",
    "        time = camera.get(cv2.CAP_PROP_POS_MSEC) #Current position of the video file in milliseconds.\n",
    "        ## downsample frame\n",
    "        #frame = cv2.resize(frame,None,fx=downsample_rate, fy=downsample_rate, interpolation = cv2.INTER_LINEAR)\n",
    "        # crop frame\n",
    "        frame_cropped = frame[top_edge:down_edge,left_edge:right_edge,:].copy() # must use copy()\n",
    "\n",
    "        # 1. Whole Interest Area\n",
    "        # Output keypoints and the image with the human skeleton blended on it\n",
    "        #    (num_people, 25_keypoints, x_y_confidence) = keypoints_whole_interest_area.shape\n",
    "        keypoints_whole_interest_area, output_image_whole_interest_area = openpose.forward(frame_cropped, True)\n",
    "\n",
    "        # 2. Core Interest Area\n",
    "        core_interest_area_mask = np.zeros(frame_cropped.shape[:2], np.uint8)\n",
    "        cv2.drawContours(core_interest_area_mask, [core_interest_area_polygon], -1, (255, 255, 255), -1, cv2.LINE_AA)\n",
    "        core_interest_area = cv2.bitwise_and(output_image_whole_interest_area, frame_cropped, mask=core_interest_area_mask)\n",
    "\n",
    "        # 3. Margin Interest Area\n",
    "        margin_interest_area = cv2.bitwise_xor(output_image_whole_interest_area, core_interest_area)\n",
    "        # TODO: infer occupancy from \"keypoints_whole_interest_area\"\n",
    "\n",
    "        # draw the text and timestamp on the frame\n",
    "        occupancy_whole = keypoints_whole_interest_area.shape[0]\n",
    "        occupancy_core = 0\n",
    "        occupancy_margin = 0\n",
    "        for people in keypoints_whole_interest_area:\n",
    "            # Sort all keypoints and pick up the one with the highest confidence\n",
    "            # Meaning of keypoints (https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md)\n",
    "            ordered_keypoints = people[people[:,2].argsort(),:] # increasing order\n",
    "            x, y = ordered_keypoints[-1][:2]\n",
    "            #pdb.set_trace()\n",
    "            # Choose the one with higher confidence to calculatate occupancy and location\n",
    "            if cv2.pointPolygonTest(core_interest_area_polygon, (x, y), False) == 1:\n",
    "                occupancy_core += 1\n",
    "            else:\n",
    "                occupancy_margin += 1\n",
    "\n",
    "        cv2.drawContours(output_image_whole_interest_area, [core_interest_area_polygon], -1, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(output_image_whole_interest_area, \"Whole Occupancy: {}, Core Occupancy: {}, Margin Occupancy: {}\".format(occupancy_whole, occupancy_core, occupancy_margin), (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.putText(core_interest_area, \"Core Occupancy: {}\".format(occupancy_core), (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.putText(margin_interest_area, \"Margin Occupancy: {}\".format(occupancy_margin), (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        # save estimated occupancy data\n",
    "        fieldnames = ['Time', 'Occupancy']\n",
    "        with open(out_occupancy_whole, 'a') as csv_datafile:\n",
    "            writer = csv.DictWriter(csv_datafile, fieldnames = fieldnames)\n",
    "            writer.writerow({'Time':time, 'Occupancy': occupancy_whole})\n",
    "        with open(out_occupancy_core, 'a') as csv_datafile:\n",
    "            writer = csv.DictWriter(csv_datafile, fieldnames = fieldnames)\n",
    "            writer.writerow({'Time':time, 'Occupancy': occupancy_core})\n",
    "        with open(out_occupancy_margin, 'a') as csv_datafile:\n",
    "            writer = csv.DictWriter(csv_datafile, fieldnames = fieldnames)\n",
    "            writer.writerow({'Time':time, 'Occupancy': occupancy_margin})\n",
    "        # save processed videos\n",
    "        out_camera_frame_whole.write(output_image_whole_interest_area)\n",
    "    else:\n",
    "        # Pass this frame if cannot grab an image.\n",
    "        print('Frame: {}, grabbed={} and frame={}'.format(frame_count, grabbed, frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def subplot_estimated_occupancy(occupancy_whole, occupancy_core, occupancy_margin, fig_filename, smooth_flag = False):\n",
    "    \"\"\"\n",
    "    Plot and save estimated occupancy in Three Interest Area.\n",
    "    Args:\n",
    "        occupancy_whole (pd.DataFrame): occupancy in Whole Interest Area\n",
    "        occupancy_core (pd.DataFrame): occupancy in Core Interest Area\n",
    "        occupancy_margin (pd.DataFrame): occupancy in Margin Interest Area\n",
    "        fig_filename (string): filename of the saved figure\n",
    "        smooth_flag (bool): indicates whether the occupancy is smoothened\n",
    "    \"\"\"\n",
    "    ymin = 0\n",
    "    ymax = 20\n",
    "    ystep = 4\n",
    "    lw=1.5\n",
    "    plt.figure()\n",
    "    # Whole Interest Area\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(occupancy_whole['Time']/1000, occupancy_whole['Occupancy'], 'b-', lw, alpha=0.6)\n",
    "    plt.xlabel('time/second')\n",
    "    plt.ylabel('# of visitors')\n",
    "    plt.ylim(ymin, ymax)\n",
    "    plt.yticks(np.arange(ymin,ymax,ystep))\n",
    "    if smooth_flag == False:\n",
    "        plt.title('Estimated # of visitors in Whole Interest Area')\n",
    "    else:\n",
    "        plt.title('Smooth Estimated # of visitors in Whole Interest Area')\n",
    "    plt.grid(True, linestyle=':')\n",
    "\n",
    "    # Core Interest Area\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(occupancy_core['Time']/1000, occupancy_core['Occupancy'], 'r-', lw, alpha=0.6)\n",
    "    plt.xlabel('time/second')\n",
    "    plt.ylabel('# of visitors')\n",
    "    plt.ylim(ymin, ymax)\n",
    "    plt.yticks(np.arange(ymin,ymax,ystep))\n",
    "    plt.title('Estimated # of visitors in Core Interest Area')\n",
    "    if smooth_flag == False:\n",
    "        plt.title('Estimated # of visitors in Core Interest Area')\n",
    "    else:\n",
    "        plt.title('Smooth Estimated # of visitors in Core Interest Area')\n",
    "    plt.grid(True, linestyle=':')\n",
    "    \n",
    "    # Margin Interest Area\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(occupancy_margin['Time']/1000, occupancy_margin['Occupancy'], 'g-', lw, alpha=0.6)\n",
    "    plt.xlabel('time/second')\n",
    "    plt.ylabel('# of visitors')\n",
    "    plt.ylim(ymin, ymax)\n",
    "    plt.yticks(np.arange(ymin,ymax,ystep))\n",
    "    if smooth_flag == False:\n",
    "        plt.title('Estimated # of visitors in Margin Interest Area')\n",
    "    else:\n",
    "        plt.title('Smooth Estimated # of visitors in Margin Interest Area')\n",
    "    plt.grid(True, linestyle=':')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_filename, dpi = 300)\n",
    "\n",
    "def plot_estimated_occupancy(occupancy_whole, occupancy_core, occupancy_margin, fig_filename, smooth_flag = False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        \n",
    "        smooth_flag (bool): indicates whether the occupancy is smoothened\n",
    "    \"\"\"\n",
    "    ymin=0\n",
    "    ymax=20\n",
    "    ystep=4\n",
    "\n",
    "    plt.figure()\n",
    "    # Whole Interest Area\n",
    "    plt.plot(occupancy_whole['Time']/1000, occupancy_whole['Occupancy'], 'r-', lw=1.5, alpha=0.6)\n",
    "    # Core Interest Area\n",
    "    plt.plot(occupancy_core['Time']/1000, occupancy_core['Occupancy'], 'g-', lw=1.5, alpha=0.6)\n",
    "    # Margin Interest Area\n",
    "    plt.plot(occupancy_margin['Time']/1000, occupancy_margin['Occupancy'], 'b-', lw=1.5, alpha=0.6)\n",
    "    plt.legend(('Whole Interest Area','Core Interest Area','Margin Interest Area'))\n",
    "\n",
    "    plt.xlabel('time/second')\n",
    "    plt.ylabel('# of visitors')\n",
    "    plt.ylim(ymin, ymax, ystep)\n",
    "    if smooth_flag == False:\n",
    "        plt.title('Estimated # of visitors in Three Interest Areas')\n",
    "    else:\n",
    "        plt.title('Smooth Estimated # of visitors in Three Interest Areas')\n",
    "    plt.grid(True, linestyle=':')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_filename, dpi = 300)\n",
    "\n",
    "def moving_smoothing(values, window_size, smooth_type='mode', stride = 1):\n",
    "    \"\"\"\n",
    "    Smoothen estimated occupancy.\n",
    "    Args:\n",
    "        values (pandas.DataFrame): \n",
    "            values['Time']: time in millisecond\n",
    "            values['Occupancy']: estimated # of visitors\n",
    "        window_size(int): the size of sliding window\n",
    "        smooth_type (string): \n",
    "            1. 'mode'\n",
    "            2. 'mean'\n",
    "            3. 'min'\n",
    "            4. 'median'\n",
    "        stride (int): the stride between two consecutive windows\n",
    "    Returns:\n",
    "        smooth_time (np.array): smooth time i.e. the max time in each window\n",
    "        smooth_occupancy (np.array): smooth occupancy i.e. the mode occupancy in each window\n",
    "    \"\"\"\n",
    "    group_time = []\n",
    "    group_occupancy = []\n",
    "    for i in range(0, math.ceil((len(values['Time'])-window_size+1)/stride)):\n",
    "        group_time.append(values['Time'][i:i+window_size])\n",
    "        group_occupancy.append(values['Occupancy'][i:i+window_size])\n",
    "   \n",
    "    smooth_time = []\n",
    "    smooth_occupancy = []\n",
    "    for i in range(len(group_time)):\n",
    "        smooth_time.append(min(group_time[i])) # max time in the group\n",
    "        if smooth_type == 'mode':\n",
    "            smooth_occupancy.append(mode(group_occupancy[i])[0][0]) # mode occupancy in the group\n",
    "        elif smooth_type == 'mean':\n",
    "            #smooth_occupancy.append(np.round(np.mean(group_occupancy[i])))\n",
    "            smooth_occupancy.append(np.mean(group_occupancy[i]))\n",
    "        elif smooth_type == 'min':\n",
    "            #smooth_occupancy.append(np.round(np.min(group_occupancy[i])))\n",
    "            smooth_occupancy.append(np.min(group_occupancy[i]))\n",
    "        elif smooth_type == 'median':\n",
    "            #smooth_occupancy.append(np.round(np.median(group_occupancy[i])))\n",
    "            smooth_occupancy.append(np.median(group_occupancy[i]))\n",
    "        else:\n",
    "            print('Please choose a proper smooth_type.')\n",
    "    smooth_values = pd.DataFrame(data={'Time': np.array(smooth_time),\n",
    "                                       'Occupancy': np.array(smooth_occupancy,dtype=int)})\n",
    "    return smooth_values#np.array(smooth_time), np.array(smooth_occupancy)\n",
    "\n",
    "def interpret_senario(occupancy_whole, occupancy_core, occupancy_margin, senarios_truth_table):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        occupancy_whole (pd.DataFrame): estimation of coccupancy in whole intrest area\n",
    "        occupancy_core (pd.DataFrame): estimation of coccupancy in core intrest area\n",
    "        occupancy_margin (pd.DataFrame): estimation of coccupancy in margin intrest area\n",
    "        senarios_truth_table (pandas.DataFrame): senarios truth table which has information on\n",
    "            how to interpret senario.\n",
    "    Returns:\n",
    "        senario_sequence (np.array): sequnce of interpreted senario discription according to \"Senario Truth Value Table\"\n",
    "        event_sequence (np.array): sequence of interpreted senario code according to \"Senario Truth Value Table\"\n",
    "            Note: Different from \"Senario Truth Value Table\", in this sequence we convert all impossible cases into 0 rather than their original senario code.\n",
    "        event_time (np.array): the time of each event in millisecond.\n",
    "    \"\"\"\n",
    "    senario_sequence = []\n",
    "    event_sequence = []\n",
    "    event_time = []\n",
    "    for i in range(len(occupancy_whole['Occupancy'])-1):\n",
    "        change_x = occupancy_core['Occupancy'][i+1] - occupancy_core['Occupancy'][i]\n",
    "        change_y = occupancy_margin['Occupancy'][i+1] - occupancy_margin['Occupancy'][i]\n",
    "        change_z = occupancy_whole['Occupancy'][i+1] - occupancy_whole['Occupancy'][i]\n",
    "        # code: \n",
    "        #    0: hold\n",
    "        #    1: increase\n",
    "        #    2: decrease\n",
    "        if change_x == 0:\n",
    "            x = 0\n",
    "        elif change_x > 0:\n",
    "            x = 1\n",
    "        elif change_x < 0:\n",
    "            x = 2\n",
    "\n",
    "        if change_y == 0:\n",
    "            y = 0\n",
    "        elif change_y > 0:\n",
    "            y = 1\n",
    "        elif change_y < 0:\n",
    "            y = 2\n",
    "\n",
    "        if change_z == 0:\n",
    "            z = 0\n",
    "        elif change_z > 0:\n",
    "            z = 1\n",
    "        elif change_z < 0:\n",
    "            z = 2\n",
    "        # convert ternary to decimal\n",
    "        senario_index = z + y*3 + x*3^2\n",
    "        senario_sequence.append(senarios_truth_table['Explanation'][senario_index])\n",
    "        if senarios_truth_table['Truth value'][senario_index] == 0:\n",
    "            # convert all impossible cases into 0\n",
    "            event_sequence.append(0)\n",
    "            #event_sequence.append(senario_index)\n",
    "        else:\n",
    "            event_sequence.append(senario_index)\n",
    "        event_time.append(occupancy_whole['Time'][i])\n",
    "    return np.array(senario_sequence), np.array(event_sequence), np.array(event_time)\n",
    "\n",
    "def plot_detected_interesting_event(senario_sequence, event_sequence, event_time, fig_filename):\n",
    "    ymin = 0\n",
    "    ymax = 26.0005\n",
    "    ystep = 1\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(event_time/1000, event_sequence)\n",
    "    plt.xlabel('time/second')\n",
    "    plt.ylabel('Event Description')\n",
    "    plt.ylim(ymin, ymax)\n",
    "    plt.yticks(np.arange(ymin,ymax,ystep), senarios_truth_table['Explanation'],\n",
    "               rotation=45, fontsize = 6)\n",
    "    ax2 = plt.twinx()\n",
    "    plt.ylabel('Event Code')\n",
    "    plt.yticks(np.arange(ymin,ymax,ystep), np.arange(ymin,ymax,ystep))\n",
    "    plt.title('Detected Interesting Events')\n",
    "    plt.grid(True, linestyle=':')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_filename, dpi = 300)\n",
    "\n",
    "def tag_interesting_event_description_on_video(video_filename,\n",
    "                                              smooth_type, window_size, stride,\n",
    "                                              senario_sequence, event_sequence, event_time):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        video_filename (string): filename of video\n",
    "        smooth_type (string): smooth type (hyper-parameter of smooth method)\n",
    "        window_size (int): size of smooth window (hyper-parameter of smooth method)\n",
    "        stride (int): stride size (hyper-parameter of smooth method)\n",
    "        senario_sequence (np.array): sequnce of interpreted senario discription according to \"Senario Truth Value Table\"\n",
    "        event_sequence (np.array): sequence of interpreted senario code according to \"Senario Truth Value Table\"\n",
    "            Note: Different from \"Senario Truth Value Table\", in this sequence we convert all impossible cases into 0 rather than their original senario code.\n",
    "        event_time (np.array): the time of each event in millisecond.\n",
    "    \"\"\"\n",
    "    camera = cv2.VideoCapture(video_filename)\n",
    "    (grabbed, frame) = camera.read()\n",
    "    fheight, fwidth, channels= frame.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out_tagged_camera_frame = cv2.VideoWriter(video_filename.split('.avi')[0]+'_tagged_smooth_type_{}_window_size_{}_stride_{}.avi'.format(smooth_type,window_size,stride),fourcc, camera.get(cv2.CAP_PROP_FPS), (fwidth,fheight))\n",
    "    # loop over the frames of the video\n",
    "    total_frame_number = camera.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    max_line_character_num = 60 # 60 characters each line\n",
    "    detected_event_time = 0\n",
    "    detected_event_senario = ''\n",
    "    line_num = 1\n",
    "    for frame_count in range(len(event_time)):\n",
    "        if frame_count % 200 == 0:\n",
    "            print('Processing frame: {}'.format(frame_count))\n",
    "        (grabbed, frame) = camera.read()\n",
    "        if grabbed == True:\n",
    "            cv2.putText(frame, \"smooth_type: {}, window_size: {}, stride: {}.\".format(smooth_type,window_size,stride), (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "            time = camera.get(cv2.CAP_PROP_POS_MSEC) #Current position of the video file in milliseconds.\n",
    "            event_index = frame_count\n",
    "            if event_sequence[event_index] != 0: # 0 means 'impossible event'\n",
    "                detected_event_time = time\n",
    "                detected_event_senario = senario_sequence[event_index]\n",
    "                cv2.putText(frame, \"Detect Interesting Event at: {}s.\".format(int(detected_event_time/1000)), (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                line_num = np.ceil(len(detected_event_senario)/max_line_character_num)\n",
    "                for i in range(int(line_num)):\n",
    "                    if i < line_num:\n",
    "                        cv2.putText(frame, \"{}\".format(detected_event_senario[i*max_line_character_num:(i+1)*max_line_character_num]), (10, 180+30*(i)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                    else:\n",
    "                        cv2.putText(frame, \"{}\".format(detected_event_senario[i*max_line_character_num:end]), (10, 180+30*(i)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "            else: # repeat text from last detected event\n",
    "                cv2.putText(frame, \"Detect Interesting Event at:{}s\".format(int(detected_event_time/1000)), (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                for i in range(int(line_num)):\n",
    "                    if i < line_num:\n",
    "                        cv2.putText(frame, \"{}\".format(detected_event_senario[i*max_line_character_num:(i+1)*max_line_character_num]), (10, 180+30*(i)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                    else:\n",
    "                        cv2.putText(frame, \"{}\".format(detected_event_senario[i*max_line_character_num:end]), (10, 180+30*(i)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "            # save processed videos\n",
    "            out_tagged_camera_frame.write(frame)\n",
    "        else:\n",
    "            # Pass this frame if cannot grab an image.\n",
    "            print('Frame: {}, grabbed={} and frame={}'.format(frame_count, grabbed, frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frame: 0\n",
      "Processing frame: 200\n",
      "Processing frame: 400\n",
      "Processing frame: 600\n",
      "Processing frame: 800\n",
      "Processing frame: 1000\n",
      "Processing frame: 1200\n",
      "Processing frame: 1400\n",
      "Processing frame: 1600\n",
      "Processing frame: 1800\n",
      "Processing frame: 2000\n",
      "Processing frame: 2200\n",
      "Processing frame: 2400\n",
      "Processing frame: 2600\n",
      "Processing frame: 2800\n",
      "Processing frame: 3000\n",
      "Processing frame: 3200\n",
      "Processing frame: 3400\n",
      "Processing frame: 3600\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "#    Smoothen Estimated Occupancy, then detect interesting event       #\n",
    "########################################################################\n",
    "\n",
    "# read estimated occupancy in Three Interest Areas\n",
    "occupancy_whole = pd.read_csv(out_occupancy_whole)\n",
    "occupancy_core = pd.read_csv(out_occupancy_core)\n",
    "occupancy_margin = pd.read_csv(out_occupancy_margin)\n",
    "\n",
    "# save plot of estimated occupancy in Three Interest Areas\n",
    "fig_filename = os.path.join(args['output_directory'], '{}_Subplot_Estimated_Occupancy.png'.format(file_name))\n",
    "subplot_estimated_occupancy(occupancy_whole, occupancy_core, occupancy_margin, fig_filename)\n",
    "fig_filename = os.path.join(args['output_directory'], '{}_Plot_Estimated_Occupancy.png'.format(file_name))\n",
    "plot_estimated_occupancy(occupancy_whole, occupancy_core, occupancy_margin, fig_filename)\n",
    "\n",
    "# smoothen\n",
    "window_size = 25\n",
    "smooth_type='mean'\n",
    "stride = 1\n",
    "smooth_occupancy_whole = moving_smoothing(occupancy_whole, window_size, smooth_type)\n",
    "smooth_occupancy_core = moving_smoothing(occupancy_core, window_size, smooth_type)\n",
    "smooth_occupancy_margin = moving_smoothing(occupancy_margin, window_size, smooth_type)\n",
    "\n",
    "fig_filename = os.path.join(args['output_directory'], '{}_Subplot_Smooth_Estimated_Occupancy.png'.format(file_name))\n",
    "subplot_estimated_occupancy(smooth_occupancy_whole,\n",
    "                            smooth_occupancy_core,\n",
    "                            smooth_occupancy_margin, \n",
    "                            fig_filename, \n",
    "                            smooth_flag = True)\n",
    "fig_filename = os.path.join(args['output_directory'], '{}_Plot_Smooth_Estimated_Occupancy.png'.format(file_name))\n",
    "plot_estimated_occupancy(smooth_occupancy_whole,\n",
    "                         smooth_occupancy_core,\n",
    "                         smooth_occupancy_margin, \n",
    "                         fig_filename,\n",
    "                         smooth_flag = True)\n",
    "\n",
    "# load Senario Truth Table\n",
    "senarios_truth_table = pd.read_csv('analize_visitor_in_and_out_senario_truth_table.csv')\n",
    "\n",
    "# Interpret\n",
    "senario_sequence, event_sequence, event_time = interpret_senario(smooth_occupancy_core, \n",
    "                                                                 smooth_occupancy_margin, \n",
    "                                                                 smooth_occupancy_whole, \n",
    "                                                                 senarios_truth_table)\n",
    "# Plot interesting events\n",
    "fig_filename = os.path.join(args['output_directory'], '{}_Plot_Interesting_Event_smooth_type_{}_window_size_{}_stride{}'.format(file_name, smooth_type, window_size, stride))\n",
    "plot_detected_interesting_event(senario_sequence, event_sequence, event_time, fig_filename)\n",
    "\n",
    "# Tag\n",
    "tag_interesting_event_description_on_video(output_video_filename, \n",
    "                                           smooth_type, window_size, stride,\n",
    "                                           senario_sequence, event_sequence, event_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
