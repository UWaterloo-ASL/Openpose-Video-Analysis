{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROM Video Analyzing\n",
    "* To run this note, you need to install Openpose following [Steps to Install Openpose in ComputeCanada Cluster](https://docs.google.com/document/d/1vQCQD2iet-K1ZAjTns2aEp-q7R-w8fCQ55jVzriWLV0/edit?usp=sharing)\n",
    "* To set up Jupyter Notebook and submit jobs in Compute Canada Clusters, please refer to [Steps to Use Openpose Analyzing Video](https://docs.google.com/document/d/1tozpc-KpAHVjQOx5XkW0pDYbd8IKKCD45Omk-RvFNcg/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('module load nixpkgs/16.09  gcc/5.4.0  cuda/8.0.44  cudnn/7.0 opencv/3.3.0  boost/1.65.1 openblas/0.2.20 hdf5/1.8.18 leveldb/1.18 mkl-dnn/0.14 python/3.5.2')\n",
    "os.system('cd ~')\n",
    "os.system('source openposeEnv_Python3/bin/activate')\n",
    "os.system('export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/openpose_python_lib/lib:$HOME/openpose_python_lib/python/openpose:$HOME/caffe/build/lib:/cvmfs/soft.computecanada.ca/easybuild/software/2017/avx2/Compiler/gcc5.4/boost/1.65.1/lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Python\n",
    "# It requires OpenCV installed for Python\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "from sys import platform\n",
    "import argparse\n",
    "\n",
    "import pdb\n",
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "# Remember to add your installation path here\n",
    "# Option b\n",
    "# If you run `make install` (default path is `/usr/local/python` for Ubuntu), you can also access the OpenPose/python module from there. This will install OpenPose and the python library at your desired installation path. Ensure that this is in your python path in order to use it.\n",
    "sys.path.insert(0,r'/home/lingheng/openpose_python_lib/python/openpose') \n",
    "\n",
    "# Parameters for OpenPose. Take a look at C++ OpenPose example for meaning of components. Ensure all below are filled\n",
    "try:\n",
    "    from openpose import *\n",
    "except:\n",
    "    raise Exception('Error: OpenPose library could not be found. Did you enable `BUILD_PYTHON` in CMake and have this Python script in the right folder?')\n",
    "params = dict()\n",
    "params[\"logging_level\"] = 3\n",
    "params[\"output_resolution\"] = \"-1x-1\"\n",
    "params[\"net_resolution\"] = \"-1x368\" # if crop video, this should be changged and must be mutplies of 16.\n",
    "params[\"model_pose\"] = \"BODY_25\"\n",
    "params[\"alpha_pose\"] = 0.6\n",
    "params[\"scale_gap\"] = 0.3\n",
    "params[\"scale_number\"] = 1\n",
    "params[\"render_threshold\"] = 0.05\n",
    "# If GPU version is built, and multiple GPUs are available, set the ID here\n",
    "params[\"num_gpu_start\"] = 0\n",
    "params[\"disable_blending\"] = False\n",
    "# Ensure you point to the correct path where models are located\n",
    "params[\"default_model_folder\"] = \"/home/lingheng/openpose/models/\"\n",
    "# Construct OpenPose object allocates GPU memory\n",
    "openpose = OpenPose(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw frames per second: 23.209837217316796\n",
      "Frame width:576, Frame height:486.\n"
     ]
    }
   ],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "args = dict()\n",
    "args['video']='/home/lingheng/project/lingheng/ROM_Video_Process/ROM_raw_videos/Sep_12_2pm_3pm.mp4'\n",
    "args['output_directory']='/home/lingheng/project/lingheng/ROM_Video_Process/ROM_processed_videos_2'\n",
    "\n",
    "if args.get(\"video\", None) is None:\n",
    "    raise Error(\"No input video!!\")\n",
    "# otherwise, we are reading from a video file\n",
    "else:\n",
    "    camera = cv2.VideoCapture(args[\"video\"])\n",
    "# frames per second (fps) in the raw video\n",
    "fps = camera.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = 1\n",
    "print(\"Raw frames per second: {0}\".format(fps))\n",
    "# prepare to save video\n",
    "(grabbed, frame) = camera.read()\n",
    "# downsample frame\n",
    "downsample_rate = 0.5\n",
    "frame = cv2.resize(frame,None,fx=downsample_rate, fy=downsample_rate, interpolation = cv2.INTER_LINEAR)\n",
    "# crop frame\n",
    "original_h, original_w, channels= frame.shape\n",
    "top_edge = int(original_h*(1/10))\n",
    "down_edge = int(original_h*1)\n",
    "left_edge = int(original_w*(1/5))\n",
    "right_edge = int(original_w*(4/5))\n",
    "frame_cropped = frame[top_edge:down_edge,left_edge:right_edge,:].copy() # must use copy(), otherwise slice only return address i.e. not hard copy\n",
    "\n",
    "fheight, fwidth, channels = frame_cropped.shape\n",
    "#pdb.set_trace()\n",
    "print(\"Frame width:{}, Frame height:{}.\".format(fwidth , fheight))\n",
    "\n",
    "# get output file name\n",
    "file_path = args[\"video\"].split('/')\n",
    "file_name, _= file_path[-1].split('.')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out_camera_frame = cv2.VideoWriter(os.path.join(args['output_directory'],'{}_processed.avi'.format(file_name)),fourcc, fps, (fwidth,fheight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83647.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camera.get(cv2.CAP_PROP_FRAME_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the frames of the video\n",
    "frame_count = camera.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "while True:\n",
    "    (grabbed, frame) = camera.read()\n",
    "    frame_count += 1\n",
    "    time = (1/fps)*1000*frame_count #time after \"frame_count\" frame\n",
    "    # if the frame could not be grabbed, then we have reached the end\n",
    "    # of the video\n",
    "    if not grabbed:\n",
    "        print('Process Done!')\n",
    "        break\n",
    "    # downsample frame\n",
    "    frame = cv2.resize(frame,None,fx=downsample_rate, fy=downsample_rate, interpolation = cv2.INTER_LINEAR)\n",
    "    # crop frame\n",
    "    frame_cropped = frame[top_edge:down_edge,left_edge:right_edge,:].copy() # must use copy()\n",
    "    #pdb.set_trace()\n",
    "    # Output keypoints and the image with the human skeleton blended on it\n",
    "    keypoints, output_image = openpose.forward(frame_cropped, True)\n",
    "    # Print the human pose keypoints, i.e., a [#people x #keypoints x 3]-dimensional numpy object with the keypoints of all the people on that image\n",
    "    #print(keypoints.shape)\n",
    "\n",
    "    # draw the text and timestamp on the frame\n",
    "    occupancy = keypoints.shape[0]\n",
    "    cv2.putText(output_image, \"Current Occupancy: {}\".format(occupancy), (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    with open(os.path.join(args['output_directory'],'{}_processed_occupancy.txt'.format(file_name)), \"a+\") as fh:\n",
    "       fh.write(\"{},{}\\r\".format(time, occupancy))\n",
    "    #Tracer()()\n",
    "    # save video\n",
    "    #cv2.imwrite('messigray_1gpu_cropped2.png',output_image)\n",
    "    #cv2.imwrite('messigray_1gpu_cropped2_frame.png',frame_cropped)\n",
    "    out_camera_frame.write(output_image)\n",
    "    # waite for a pressed key\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-16a4fe7b95ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Output keypoints and the image with the human skeleton blended on it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mkeypoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenpose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_cropped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Print the human pose keypoints, i.e., a [#people x #keypoints x 3]-dimensional numpy object with the keypoints of all the people on that image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#print(keypoints.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openpose_python_lib/python/openpose/openpose.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, display)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mdisplayImage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplayImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loop over the frames of the video\n",
    "total_frame_number = camera.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "for frame_count in range(int(total_frame_number)):\n",
    "    (grabbed, frame) = camera.read()\n",
    "    time = camera.get(cv2.CAP_PROP_POS_MSEC) #Current position of the video file in milliseconds.\n",
    "    # if the frame could not be grabbed, then we have reached the end\n",
    "    # of the video\n",
    "    if not grabbed:\n",
    "        print('Process Done!')\n",
    "        break\n",
    "    # downsample frame\n",
    "    frame = cv2.resize(frame,None,fx=downsample_rate, fy=downsample_rate, interpolation = cv2.INTER_LINEAR)\n",
    "    # crop frame\n",
    "    frame_cropped = frame[top_edge:down_edge,left_edge:right_edge,:].copy() # must use copy()\n",
    "    #pdb.set_trace()\n",
    "    # Output keypoints and the image with the human skeleton blended on it\n",
    "    keypoints, output_image = openpose.forward(frame_cropped, True)\n",
    "    # Print the human pose keypoints, i.e., a [#people x #keypoints x 3]-dimensional numpy object with the keypoints of all the people on that image\n",
    "    #print(keypoints.shape)\n",
    "\n",
    "    # draw the text and timestamp on the frame\n",
    "    occupancy = keypoints.shape[0]\n",
    "    cv2.putText(output_image, \"Current Occupancy: {}\".format(occupancy), (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    with open(os.path.join(args['output_directory'],'{}_processed_occupancy.txt'.format(file_name)), \"a+\") as fh:\n",
    "       fh.write(\"{},{}\\r\".format(time, occupancy))\n",
    "    #Tracer()()\n",
    "    # save video\n",
    "    #cv2.imwrite('messigray_1gpu_cropped2.png',output_image)\n",
    "    #cv2.imwrite('messigray_1gpu_cropped2_frame.png',frame_cropped)\n",
    "    out_camera_frame.write(output_image)\n",
    "    # waite for a pressed key\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91771.4320896147"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camera.get(cv2.CAP_PROP_POS_MSEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = (1/fps)*1000*frame_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91685.2617308451"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
